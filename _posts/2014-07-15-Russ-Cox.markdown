---
layout: post
title:  Russ Cox
categories: concurrency c systems-programming
image: /images/russ.cox.jpg
---
Russ Cox works at Google on the Go programming language. He previously worked at Bell Labs where he contributed to Plan9.

<!-- more -->
<h1 id="who-are-you">Who are you?</h1>

<p>
I&rsquo;m a programmer.

<p>
I write programs.
I worked on <a href="http://plan9.bell-labs.com/plan9/">Plan 9 from Bell Labs</a> for about a decade,
writing kernel code, networked servers, file systems, and a bit of graphics code.
Now I work at Google, where I&rsquo;m one of the lead developers of the <a href="http://golang.org/">Go programming language</a>.
Go has turned out to be a nice general-purpose language, but its original design target was
concurrent networked servers, the kinds of programs we were writing for Plan 9 and for Google.

<p>
I also write about programs. My most well-known articles are about
<a href="http://swtch.com/~rsc/regexp/">implementing regular expressions</a>,
putting a <a href="http://research.swtch.com/zip">zip file inside itself</a>, and
<a href="http://research.swtch.com/qart">making pictures in QR codes</a>.
I used Go for all three.

<p>
<h1 id="whats-the-most-interesting-bug-youve-met">What’s the most interesting bug you’ve met?</h1>

<p>
To me, the most interesting bugs are the ones that reveal fundamental, subtle misunderstandings about the way a program works.
A good bug is like a good science experiment:
through it, you learn something unexpected
about the virtual world you are exploring.

<p>
About ten years ago I was working on a networked server that used threads,
coordinating with locks and condition variables.
This server was part of Plan 9 and was written in C.
Occasionally it would crash inside <code>malloc</code>,
which usually means some kind of memory corruption due
to a write-after-free error.
One day, while benchmarking with the bulk of the server disabled,
I was lucky enough to have the crash happen reproducibly.
The server being mostly disabled gave me a head start in isolating the bug,
and the reproducibility made it possible to cut code out, piece by piece,
until one section was very clearly implicated.

<p>
The code in question was cleaning up after a client that had recently disconnected.
In the server, there is a per-client data structure shared by two threads:
the thread R reads from the client connection, and the thread W writes to it.
R notices the disconnect as an EOF from a read, notifies W,
waits for an acknowledgement from W, and then frees the per-client structure.

<p>
To acknowledge the disconnect, W ran code like:

<pre>
qlock(&conn->lk);
conn->writer_done = 1;
qsignal(&conn->writer_ack);
qunlock(&conn->lk);
thread_exit();
</pre>

<p>
And to wait for the acknowledgement, R ran code like:

<pre>
qlock(&conn->lk);
while(!conn->writer_done)
    qwait(&conn->writer_ack);

// The writer is done, and so are we:
// free the connection.
free(conn);
</pre>

<p>
This is a standard locks and condition variables piece of code:
<code>qwait</code> is defined to release the lock (here, <code>conn->lk</code>),
wait, and then reacquire the lock before returning.
Once R observes that <code>writer_done</code> is set,
R knows that W is gone, so R can <code>free</code> the per-connection data structure.

<p>
R does not call <code>qunlock(&conn->lk)</code>.
My reasoning was that calling <code>qunlock</code> before <code>free</code> sends mixed messages:
<code>qunlock</code> suggests coordination with another thread using <code>conn</code>,
but <code>free</code> is only safe if no other thread is using <code>conn</code>.
W was the other thread, and W is gone.
But somehow, when I added <code>qunlock(&conn->lk)</code> before <code>free(conn)</code>, the crashes stopped. Why?

<p>
To answer that, we have to look at how locks are implemented.

<p>
Conceptually, the core of a lock is a variable with two markings <i>unlocked</i> and <i>locked</i>.
To acquire a lock, a thread checks that the core is marked <i>unlocked</i>
and, if so, marks it <i>locked</i>, in one atomic operation.
Because the operation is atomic, if two (or more) threads are attempting to acquire the lock, only one can succeed.
That thread&mdash;let&rsquo;s call it thread A&mdash;now holds the lock.
Another thread vying for the lock&mdash;thread B&mdash;sees the core is
marked <i>locked</i> and must now decide what to do. 

<p>
The first, simplest approach, is to try again, and again, and again.
Eventually thread A will release the lock (by marking the core <i>unlocked</i>),
at which point thread B&rsquo;s atomic operation will succeed.
This approach is called spinning, and a lock using this approach is called a <a href="http://en.wikipedia.org/wiki/Spinlock">spin lock</a>.

<p>
A simple spin lock implementation looks like:

<pre>
struct SpinLock
{
    int bit;
};

void
spinlock(SpinLock *lk)
{
    for(;;) {
        if(atomic_cmp_and_set(&lk->bit, 0, 1))
            return;
    }
}

void
spinunlock(SpinLock *lk)
{
    atomic_set(&lk->bit, 0);
}
</pre>

<p>
The spin lock&rsquo;s core is the <code>bit</code> field.
It is 0 or 1 to indicate unlocked or locked.
The <code>atomic_cmp_and_set</code> and <code>atomic_set</code>
use special machine instructions to manipulate <code>lk->bit</code> atomically.

<p>
Spinning only makes sense if the lock is never held for very long,
so that B&rsquo;s spin loop only executes a small number of times.
If the lock can be held for longer periods of time, spinning while it is held
wastes CPU and can interact badly with the operating system scheduler.

<p>
The second, more general approach is to maintain a queue of threads interested in acquiring the lock.
In this approach, when thread B finds the lock already held,
it adds itself to the queue and uses an operating system primitive to go to sleep.
When thread A eventually releases the lock, it checks the queue, finds B,
and uses an operating system primitive to wake B.
This approach is called queueing, and a lock using this approach is called a queue lock.
Queueing is more efficient than spinning when the lock may be held for a long time.

<p>
The queue lock&rsquo;s queue needs its own lock, almost always a spin lock.
In the library I was using, <code>qlock</code> and <code>qunlock</code> were implemented as:

<pre>
struct QLock
{
    SpinLock spin;
    Thread *owner;
    ThreadQueue queue;
};

void
qlock(QLock *lk)
{
    spinlock(&lk->spin);
    if(lk->owner == nil) {
        lk->owner = current_thread();
        spinunlock(&lk->spin);
        return;
    }
    push(&lk->queue, current_thread());
    spinunlock(&lk->spin);
    os_sleep();
}

void
qunlock(QLock *lk)
{
    Thread *t;

    spinlock(&lk->spin);
    t = pop(&lk->queue);
    lk->owner = t;
    if(t != nil)
        os_wakeup(t);
    spinunlock(&lk->spin);
}
</pre>

<p>
The queue lock&rsquo;s core is the <code>owner</code> field.
If <code>owner</code> is <code>nil</code>, the lock is unlocked;
otherwise <code>owner</code> records the thread that holds the lock.
The operations on <code>lk->owner</code> are made atomic by
holding the spin lock <code>lk->spin</code>.

<p>
Back to the bug.

<p>
The locks in the crashing code were queue locks.
The acknowledgement protocol between R and W sets up a race between W&rsquo;s call to <code>qunlock</code> and R&rsquo;s call to <code>qlock</code> (either the explicit call in the code or the implicit call inside <code>qwait</code>). 
Which call happens first?

<p>
If W&rsquo;s <code>qunlock</code> happens first, then R&rsquo;s <code>qlock</code> finds the lock unlocked, locks it, and everything proceeds uneventfully.

<p>
If R&rsquo;s <code>qlock</code> happens first, it finds the lock held by W, so it adds R to the queue and puts R to sleep. Then W&rsquo;s <code>qunlock</code> executes. It sets the owner to R, wakes up R, and unlocks the spin lock.
By the time W unlocks the spin lock, R may have already started running, and R may have already called <code>free(conn)</code>. The <code>spinunlock</code>&rsquo;s <code>atomic_set</code> writes a zero to <code>conn->lk.spin.bit</code>.
That&rsquo;s the write-after-free, and if the memory allocator didn&rsquo;t want a zero there, the zero may cause a crash (or a memory leak, or some other behavior).

<p>
But is the server code wrong or is <code>qunlock</code> wrong?

<p>
The fundamental misunderstanding here is in the definition of the queue lock API.
Is a queue lock required to be unlocked before being freed?
Or is a queue lock required to support being freed while locked?
I had written the queue lock routines as part of a cross-platform library
mimicking Plan 9&rsquo;s, and this question had not occurred to me when
I was writing <code>qunlock</code>.

<ul>
<li style="margin-top: 1em;">
If the queue lock must be freed only when unlocked,
then <code>qunlock</code>&rsquo;s implementation is correct
and the server must change.
If R calls <code>qunlock</code> before <code>free</code>,
then R&rsquo;s <code>qunlock</code>&rsquo;s <code>spinlock</code> must wait for W&rsquo;s <code>qunlock</code>&rsquo;s <code>spinunlock</code>,
so W will really be gone by the time R calls <code>free</code>.

<li style="margin-top: 1em;">
If the queue lock can be freed while locked, then the server is correct and
<code>qunlock</code> must change: the <code>os_wakeup</code> gives up
control of <code>lk</code> and must be delayed until after the <code>spinunlock</code>.
</ul>

<p>
The Plan 9 documentation for queue locks does not address the question directly,
but the implementation was such that freeing locked queue locks was harmless,
and since I was using my library to run unmodified Plan 9 software,
I <a href="https://code.google.com/p/plan9port/source/diff?spec=svn7bcaaad58466800dad59c670a070ff287c251da6&name=7bcaaad58466&r=7bcaaad58466800dad59c670a070ff287c251da6&format=side&path=/src/libthread/thread.c#sc_svn8f21416d6e6801f20411ef5ca984a9a7a7a93094_417">changed the lock implementation</a>
to call <code>os_wakeup</code> after <code>spinunlock</code>.
Two years later, while fixing a different bug, I defensively
<a href="https://code.google.com/p/plan9port/source/diff?spec=svn130dd046090c195aeaff27246bbe6c7fc39f2f08&name=130dd046090c&r=130dd046090c195aeaff27246bbe6c7fc39f2f08&format=side&path=/src/libventi/conn.c">changed the server implementation</a> to call
<code>qunlock</code> too, just in case.
The definition of the POSIX <a href="http://pubs.opengroup.org/onlinepubs/7908799/xsh/pthread_mutex_init.html">pthread_mutex_destroy</a>
function gives a different answer to the same design question:
&ldquo;It is safe to destroy an initialised mutex that is unlocked.
Attempting to destroy a locked mutex results in undefined behaviour.&rdquo;

<p>
What did we learn?

<p>
The rationale I gave for not calling <code>qunlock</code> before <code>free</code>
made an implicit assumption that the two were independent.
After looking inside an implementation, we can see why
the two are intertwined and why an API might specify,
as POSIX does, that you must unlock a lock before destroying it.
This is an example of implementation concerns influencing an API,
creating a &ldquo;<a href="http://en.wikipedia.org/wiki/Leaky_abstraction">leaky abstraction</a>.&rdquo;

<p>
What makes this bug interesting is that it was caused by a
complex interaction between manual memory management and
<a href="http://blog.golang.org/concurrency-is-not-parallelism">concurrency</a>.
Obviously a program must stop using a resource before freeing it.
But a concurrent program must stop all threads from
using a resource before freeing it.
On a good day, that can require bookkeeping or careful coordination
to track which threads are still using the resource.
On a bad day, that can require reading the lock implementation
to understand the exact order of operations carried
out in the different threads. 

<p>
In the modern computing world of clients and servers and clouds,
concurrency is a fundamental concern for most programs.
In that world, choosing garbage collection instead of
manual memory management eliminates a source of leaky abstractions
and makes programs simpler and easier to reason about.

<h1 id="anything-else-to-add">Anything else to add?</h1>

<p>
I started the post by saying that good bugs help you learn something unexpected
about the virtual world you are exploring.
This was especially true for Maurice Wilkes and his team, who built <a href="http://en.wikipedia.org/wiki/Electronic_Delay_Storage_Automatic_Calculator">EDSAC</a>, the first practical stored-program computer.
The first program they ran on EDSAC (printing square numbers) ran correctly,
but the second did not: the <a href="http://www.cl.cam.ac.uk/relics/elog.html">log</a> for May 7, 1949 reads &ldquo;Table of primes attempted - programme incorrect.&rdquo;
That was a Saturday, making this the first weekend spent working on a buggy program.

<p>
What did they learn? Wilkes later recalled, 
<blockquote>
&ldquo;By June 1949, people had begun to realize that it was not so easy to get a program right as had at one time appeared. ... It was on one of my journeys between the EDSAC room and the punching equipment that the realization came over me with full force that a good part of the remainder of my life was going to be spent in finding errors in my own programs.&rdquo; (<a href="http://books.google.com/books?id=9Uc4AQAAIAAJ">Wilkes</a>, p. 145)
</blockquote>

<p>
For more about this early history, see Brian Hayes&rsquo;s &ldquo;<a href="http://bit-player.org/wp-content/extras/bph-publications/Sciences-1993-07-Hayes-EDSAC.pdf">The Discovery of Debugging</a>&rdquo;
and Martin Campbell-Kelly&rsquo;s &ldquo;<a href="http://dx.doi.org/10.1109/85.194051">The Airy Tape: An Early Chapter in the History of Debugging</a>.&rdquo;

